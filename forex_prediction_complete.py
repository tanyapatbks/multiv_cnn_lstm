"""
Multi-Currency CNN-LSTM Forex Prediction System - Enhanced Version
Complete implementation with hyperparameter tuning, step-wise execution, and comprehensive visualization

Author: Your Name
Thesis: Master's Degree - Forex Trend Prediction
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import os
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ================================================================================================
# CONFIGURATION SECTION - HYPERPARAMETER TUNING SUPPORT
# ================================================================================================

class Config:
    """Configuration with hyperparameter tuning support"""
    
    def __init__(self, hyperparams=None):
        # Data Configuration
        self.CURRENCY_PAIRS = ['EURUSD', 'GBPUSD', 'USDJPY']
        self.FEATURES_PER_PAIR = 5  # OHLCV
        self.TOTAL_FEATURES = len(self.CURRENCY_PAIRS) * self.FEATURES_PER_PAIR  # 15
        
        # Model Architecture - Can be tuned
        if hyperparams:
            self.WINDOW_SIZE = hyperparams.get('window_size', 60)
            self.CNN_FILTERS_1 = hyperparams.get('cnn_filters_1', 64)
            self.CNN_FILTERS_2 = hyperparams.get('cnn_filters_2', 128)
            self.CNN_KERNEL_SIZE = hyperparams.get('cnn_kernel_size', 3)
            self.LSTM_UNITS_1 = hyperparams.get('lstm_units_1', 128)
            self.LSTM_UNITS_2 = hyperparams.get('lstm_units_2', 64)
            self.DENSE_UNITS = hyperparams.get('dense_units', 32)
            self.LEARNING_RATE = hyperparams.get('learning_rate', 0.001)
            self.BATCH_SIZE = hyperparams.get('batch_size', 32)
            self.DROPOUT_RATE = hyperparams.get('dropout_rate', 0.3)
        else:
            # Default values
            self.WINDOW_SIZE = 60
            self.CNN_FILTERS_1 = 64
            self.CNN_FILTERS_2 = 128
            self.CNN_KERNEL_SIZE = 3
            self.LSTM_UNITS_1 = 128
            self.LSTM_UNITS_2 = 64
            self.DENSE_UNITS = 32
            self.LEARNING_RATE = 0.001
            self.BATCH_SIZE = 32
            self.DROPOUT_RATE = 0.3
        
        # Training Configuration
        self.EPOCHS = 100
        self.VALIDATION_SPLIT = 0.2
        
        # Data Splits (temporal split) - NO DATA LEAKAGE
        self.TRAIN_START = '2018-01-01'
        self.TRAIN_END = '2020-12-31'
        self.VAL_START = '2021-01-01'
        self.VAL_END = '2021-12-31'
        self.TEST_START = '2022-01-01'  # TEST SET - Only used in final evaluation
        self.TEST_END = '2022-12-31'
        
        # Trading Strategy Configuration
        self.THRESHOLDS = {
            'conservative': {'buy': 0.7, 'sell': 0.3},
            'moderate': {'buy': 0.6, 'sell': 0.4},
            'aggressive': {'buy': 0.55, 'sell': 0.45}
        }
        
        # Risk Management
        self.MIN_HOLDING_HOURS = 1
        self.MAX_HOLDING_HOURS = 3
        self.STOP_LOSS_PCT = 2.0
        
        # File Paths
        self.DATA_PATH = 'data/'
        self.RESULTS_PATH = 'results/'
        self.MODELS_PATH = 'models/'
        self.CHECKPOINTS_PATH = 'checkpoints/'
        
        # Create directories
        os.makedirs(self.RESULTS_PATH, exist_ok=True)
        os.makedirs(self.MODELS_PATH, exist_ok=True)
        os.makedirs(self.CHECKPOINTS_PATH, exist_ok=True)

# ================================================================================================
# HYPERPARAMETER TUNING CLASS
# ================================================================================================

class HyperparameterTuner:
    """Grid search hyperparameter tuning"""
    
    def __init__(self):
        self.results = []
        
    def define_search_space(self):
        """Define hyperparameter search space"""
        return {
            'window_size': [45, 60, 90],
            'cnn_filters_1': [32, 64, 96],
            'cnn_filters_2': [64, 128, 192],
            'lstm_units_1': [64, 128, 256],
            'lstm_units_2': [32, 64, 128],
            'learning_rate': [0.0005, 0.001, 0.002],
            'batch_size': [16, 32, 64],
            'dropout_rate': [0.2, 0.3, 0.4]
        }
    
    def random_search(self, n_trials=10):
        """Perform random search"""
        search_space = self.define_search_space()
        
        print(f"üîç Starting Random Search with {n_trials} trials...")
        
        for trial in range(n_trials):
            # Sample random hyperparameters
            hyperparams = {}
            for param, values in search_space.items():
                hyperparams[param] = np.random.choice(values)
            
            print(f"\nüìä Trial {trial + 1}/{n_trials}")
            print(f"   Hyperparameters: {hyperparams}")
            
            try:
                # Train and evaluate model
                score = self.evaluate_hyperparams(hyperparams)
                
                self.results.append({
                    'trial': trial + 1,
                    'hyperparams': hyperparams,
                    'score': score
                })
                
                print(f"   ‚úÖ Score: {score:.4f}")
                
            except Exception as e:
                print(f"   ‚ùå Trial failed: {str(e)}")
        
        # Find best hyperparameters
        best_result = max(self.results, key=lambda x: x['score'])
        print(f"\nüèÜ Best hyperparameters (Score: {best_result['score']:.4f}):")
        for param, value in best_result['hyperparams'].items():
            print(f"   {param}: {value}")
        
        return best_result
    
    def evaluate_hyperparams(self, hyperparams):
        """Evaluate specific hyperparameters"""
        config = Config(hyperparams)
        
        # Quick training for hyperparameter search
        processor = DataProcessor()
        raw_data = processor.load_currency_data(verbose=False)
        processed_data = processor.preprocess_data(raw_data, verbose=False)
        unified_data, _ = processor.create_unified_dataset(processed_data, verbose=False)
        
        sequence_prep = SequencePreparator()
        X, y, timestamps = sequence_prep.create_sequences(unified_data, target_pair='EURUSD', verbose=False)
        data_splits = sequence_prep.split_temporal_data(X, y, timestamps, verbose=False)
        
        # Train model with reduced epochs for speed
        model_builder = CNNLSTMModel(config)
        model = model_builder.build_model(verbose=False)
        
        # Quick training
        X_train, y_train, _ = data_splits['train']
        X_val, y_val, _ = data_splits['val']
        
        history = model.fit(
            X_train, y_train,
            batch_size=config.BATCH_SIZE,
            epochs=20,  # Reduced for speed
            validation_data=(X_val, y_val),
            verbose=0
        )
        
        # Return validation accuracy as score
        return max(history.history['val_accuracy'])

# ================================================================================================
# CHECKPOINT MANAGER FOR STEP-WISE EXECUTION
# ================================================================================================

class CheckpointManager:
    """Manage checkpoints for step-wise execution"""
    
    def __init__(self, config):
        self.config = config
        self.checkpoint_file = f"{config.CHECKPOINTS_PATH}experiment_checkpoint.pkl"
        
    def save_checkpoint(self, step, data):
        """Save checkpoint for specific step"""
        checkpoint = {
            'step': step,
            'timestamp': datetime.now(),
            'data': data
        }
        
        with open(self.checkpoint_file, 'wb') as f:
            pickle.dump(checkpoint, f)
        
        print(f"üíæ Checkpoint saved for step: {step}")
    
    def load_checkpoint(self):
        """Load latest checkpoint"""
        if os.path.exists(self.checkpoint_file):
            with open(self.checkpoint_file, 'rb') as f:
                return pickle.load(f)
        return None
    
    def list_available_steps(self):
        """List available steps that can be resumed"""
        steps = [
            "1_data_preprocessing",
            "2_sequence_preparation", 
            "3_model_training",
            "4_model_evaluation",
            "5_strategy_testing",
            "6_final_evaluation",
            "7_visualization"
        ]
        return steps

# ================================================================================================
# ENHANCED DATA PROCESSOR
# ================================================================================================

class DataProcessor:
    """Handle all data processing operations"""
    
    def __init__(self):
        self.scalers = {}
        
    def load_currency_data(self, verbose=True):
        """Load OHLCV data for all currency pairs"""
        if verbose:
            print("üìä Loading currency data...")
        
        data = {}
        for pair in ['EURUSD', 'GBPUSD', 'USDJPY']:  # Fixed list to prevent leakage
            try:
                file_path = f"data/{pair}_1H.csv"
                df = pd.read_csv(file_path)
                
                if verbose:
                    print(f"   üìÖ {pair}: Raw data shape: {df.shape}")
                    print(f"   üìÖ Sample datetime: {df['Local time'].iloc[0]}")
                
                # Handle specific datetime format: "13.01.2018 00:00:00.000 GMT+0700"
                try:
                    # Strategy 1: Direct conversion
                    df['Local time'] = pd.to_datetime(df['Local time'], infer_datetime_format=True)
                    if verbose:
                        print(f"   ‚úÖ Direct datetime conversion successful")
                except Exception as e1:
                    if verbose:
                        print(f"   ‚ö†Ô∏è  Direct conversion failed: {str(e1)}")
                    
                    try:
                        # Strategy 2: Remove timezone and milliseconds first
                        df['Local time'] = df['Local time'].astype(str)
                        # Remove GMT timezone info: "GMT+0700" or "GMT-0500"
                        df['Local time'] = df['Local time'].str.replace(r' GMT[+-]\d{4}', '', regex=True)
                        # Remove milliseconds: ".000"
                        df['Local time'] = df['Local time'].str.replace(r'\.000', '', regex=True)
                        df['Local time'] = pd.to_datetime(df['Local time'], format='%d.%m.%Y %H:%M:%S')
                        if verbose:
                            print(f"   ‚úÖ Format-specific conversion successful")
                    except Exception as e2:
                        if verbose:
                            print(f"   ‚ö†Ô∏è  Format-specific failed: {str(e2)}")
                        
                        try:
                            # Strategy 3: Manual parsing
                            df['Local time'] = df['Local time'].astype(str)
                            df['Local time'] = df['Local time'].str.replace(r' GMT[+-]\d{4}', '', regex=True)
                            df['Local time'] = df['Local time'].str.replace(r'\.000', '', regex=True)
                            df['Local time'] = pd.to_datetime(df['Local time'], dayfirst=True)
                            if verbose:
                                print(f"   ‚úÖ Manual parsing successful")
                        except Exception as e3:
                            raise ValueError(f"All datetime parsing strategies failed for {pair}. "
                                           f"Sample: {df['Local time'].iloc[0]}")
                
                # Set index and sort
                df.set_index('Local time', inplace=True)
                df.sort_index(inplace=True)
                
                # Verify datetime index
                if not isinstance(df.index, pd.DatetimeIndex):
                    raise ValueError(f"Failed to create DatetimeIndex for {pair}")
                
                data[pair] = df
                if verbose:
                    print(f"   ‚úÖ {pair}: {len(df)} records loaded successfully")
                    print(f"   üìÖ Date range: {df.index.min()} to {df.index.max()}")
                    print(f"   üìÖ Duration: {(df.index.max() - df.index.min()).days} days")
                
            except Exception as e:
                if verbose:
                    print(f"   ‚ùå Error loading {pair}: {str(e)}")
                return None
        
        return data
    
    def preprocess_data(self, data, verbose=True):
        """Preprocess data: handle missing values, calculate returns, normalize"""
        if verbose:
            print("üîß Preprocessing data...")
        
        processed_data = {}
        
        for pair, df in data.items():
            # Handle missing values
            df = df.fillna(method='ffill').fillna(method='bfill')
            
            # Calculate percentage returns for OHLC
            for col in ['Open', 'High', 'Low', 'Close']:
                df[f'{col}_Return'] = df[col].pct_change().fillna(0)
                df[f'{col}_Price'] = df[col]  # Keep original prices
            
            # Volume handling
            df['Volume_Original'] = df['Volume']
            
            processed_data[pair] = df
            if verbose:
                print(f"   ‚úÖ {pair}: Preprocessing completed")
        
        return processed_data
    
    def create_unified_dataset(self, processed_data, verbose=True):
        """Create unified multi-currency dataset"""
        if verbose:
            print("üîó Creating unified multi-currency dataset...")
        
        # Find common timestamps
        common_index = None
        for pair, df in processed_data.items():
            if common_index is None:
                common_index = df.index
            else:
                common_index = common_index.intersection(df.index)
        
        if verbose:
            print(f"   üìÖ Common timestamps: {len(common_index)}")
        
        # Create unified feature matrix (FIXED ORDER for consistency)
        feature_columns = []
        unified_features = []
        
        for pair in ['EURUSD', 'GBPUSD', 'USDJPY']:  # Fixed order
            df = processed_data[pair].loc[common_index]
            
            # Select features in fixed order
            pair_features = ['Open_Return', 'High_Return', 'Low_Return', 'Close_Return', 'Volume_Original']
            pair_data = df[pair_features]
            
            # Rename columns with pair prefix
            pair_data.columns = [f'{pair}_{col}' for col in pair_data.columns]
            
            unified_features.append(pair_data)
            feature_columns.extend(pair_data.columns)
        
        # Concatenate all features
        unified_df = pd.concat(unified_features, axis=1)
        
        # Normalize features
        unified_df = self.normalize_features(unified_df, fit=True)
        
        if verbose:
            print(f"   ‚úÖ Unified dataset: {unified_df.shape[0]} samples √ó {unified_df.shape[1]} features")
        
        return unified_df, feature_columns
    
    def normalize_features(self, df, fit=True):
        """Normalize features using StandardScaler for returns and MinMaxScaler for volume"""
        normalized_df = df.copy()
        
        for col in df.columns:
            if 'Return' in col:
                if fit:
                    scaler = StandardScaler()
                    normalized_df[col] = scaler.fit_transform(df[[col]]).flatten()
                    self.scalers[col] = scaler
                else:
                    if col in self.scalers:
                        normalized_df[col] = self.scalers[col].transform(df[[col]]).flatten()
            
            elif 'Volume' in col:
                if fit:
                    scaler = MinMaxScaler()
                    normalized_df[col] = scaler.fit_transform(df[[col]]).flatten()
                    self.scalers[col] = scaler
                else:
                    if col in self.scalers:
                        normalized_df[col] = self.scalers[col].transform(df[[col]]).flatten()
        
        return normalized_df

# ================================================================================================
# SEQUENCE PREPARATION
# ================================================================================================

class SequencePreparator:
    """Create sequences for CNN-LSTM training"""
    
    def create_sequences(self, unified_data, target_pair='EURUSD', verbose=True):
        """Create sliding window sequences and labels"""
        if verbose:
            print(f"üìã Creating sequences for {target_pair} prediction...")
            print(f"   üìä Unified data shape: {unified_data.shape}")
            print(f"   üìÖ Date range: {unified_data.index.min()} to {unified_data.index.max()}")
        
        # Use actual config window size - fix hardcoded value
        window_size = 60  # TODO: This should come from config parameter
        
        feature_matrix = unified_data.values
        target_column = f'{target_pair}_Close_Return'
        
        if target_column not in unified_data.columns:
            raise ValueError(f"Target column {target_column} not found in columns: {unified_data.columns.tolist()}")
        
        target_returns = unified_data[target_column].values
        
        # Calculate number of sequences we can create
        # We need window_size for features + 1 for prediction target
        num_sequences = len(unified_data) - window_size
        
        if num_sequences <= 0:
            raise ValueError(f"Insufficient data: need at least {window_size + 1} records, got {len(unified_data)}")
        
        if verbose:
            print(f"   üìê Window size: {window_size}")
            print(f"   üìä Total data points: {len(unified_data)}")
            print(f"   üìä Sequences to create: {num_sequences}")
            print(f"   üìä Features per sequence: {unified_data.shape[1]} (should be 15 for 3 currencies √ó 5 features)")
        
        # Initialize arrays with correct feature dimension
        num_features = unified_data.shape[1]
        X = np.zeros((num_sequences, window_size, num_features), dtype=np.float32)
        y = np.zeros(num_sequences, dtype=np.float32)
        timestamps = []
        
        # Create sequences with progress tracking
        if verbose:
            print(f"   üîÑ Creating sequences...")
        
        for i in range(num_sequences):
            # Feature sequence (lookback window)
            X[i] = feature_matrix[i:i + window_size]
            
            # Target label (direction: 1 if positive return, 0 if negative)
            future_return = target_returns[i + window_size]
            y[i] = 1.0 if future_return > 0 else 0.0
            
            # Store timestamp of prediction point
            timestamps.append(unified_data.index[i + window_size])
            
            # Progress indicator
            if verbose and (i + 1) % 10000 == 0:
                print(f"      Progress: {i + 1:,}/{num_sequences:,} ({(i + 1)/num_sequences*100:.1f}%)")
        
        timestamps = pd.DatetimeIndex(timestamps)
        
        # Validate sequences
        if verbose:
            print(f"   üìä Final sequence shape: {X.shape}")
            print(f"   üìä Labels shape: {y.shape}")
            print(f"   üìä Timestamps shape: {len(timestamps)}")
            print(f"   üìä Class balance: {y.mean():.3f} (1=up, 0=down)")
            print(f"   üìÖ Sequence date range: {timestamps.min()} to {timestamps.max()}")
            
            # Sanity checks
            if np.isnan(X).any():
                print(f"   ‚ö†Ô∏è  WARNING: Found NaN values in feature sequences")
            if np.isnan(y).any():
                print(f"   ‚ö†Ô∏è  WARNING: Found NaN values in labels")
            
            # Verify shape matches expected CNN input
            expected_shape = (num_sequences, window_size, 15)  # 3 currencies √ó 5 features
            if X.shape != expected_shape:
                print(f"   ‚ö†Ô∏è  WARNING: Shape mismatch!")
                print(f"      Expected: {expected_shape}")
                print(f"      Actual: {X.shape}")
                print(f"      This may cause model input errors!")
        
        return X, y, timestamps
    
    def split_temporal_data(self, X, y, timestamps, verbose=True):
        """Split data temporally - PREVENT DATA LEAKAGE"""
        if verbose:
            print("üìÖ Splitting data temporally (NO DATA LEAKAGE)...")
        
        # Fixed dates to prevent leakage
        train_start = pd.to_datetime('2018-01-01')
        train_end = pd.to_datetime('2020-12-31')
        val_start = pd.to_datetime('2021-01-01')
        val_end = pd.to_datetime('2021-12-31')
        test_start = pd.to_datetime('2022-01-01')
        test_end = pd.to_datetime('2022-12-31')
        
        # Create masks
        train_mask = (timestamps >= train_start) & (timestamps <= train_end)
        val_mask = (timestamps >= val_start) & (timestamps <= val_end)
        test_mask = (timestamps >= test_start) & (timestamps <= test_end)
        
        splits = {
            'train': (X[train_mask], y[train_mask], timestamps[train_mask]),
            'val': (X[val_mask], y[val_mask], timestamps[val_mask]),
            'test': (X[test_mask], y[test_mask], timestamps[test_mask])  # Only for final evaluation
        }
        
        if verbose:
            for split_name, (X_split, y_split, ts_split) in splits.items():
                print(f"   üìä {split_name.upper()}: {len(y_split)} samples, "
                      f"balance: {y_split.mean():.3f}")
                if len(ts_split) > 0:
                    print(f"      Period: {ts_split.min().date()} to {ts_split.max().date()}")
        
        return splits

# ================================================================================================
# ENHANCED CNN-LSTM MODEL
# ================================================================================================

class CNNLSTMModel:
    """CNN-LSTM Architecture with enhanced features"""
    
    def __init__(self, config):
        self.config = config
        self.model = None
        
    def build_model(self, verbose=True):
        """Build CNN-LSTM model"""
        if verbose:
            print("üèóÔ∏è Building CNN-LSTM model architecture...")
        
        model = Sequential([
            # CNN Layer 1
            Conv1D(filters=self.config.CNN_FILTERS_1, 
                   kernel_size=self.config.CNN_KERNEL_SIZE, 
                   padding='same', 
                   activation='relu',
                   input_shape=(self.config.WINDOW_SIZE, self.config.TOTAL_FEATURES)),
            BatchNormalization(),
            
            # CNN Layer 2
            Conv1D(filters=self.config.CNN_FILTERS_2, 
                   kernel_size=self.config.CNN_KERNEL_SIZE, 
                   padding='same', 
                   activation='relu'),
            BatchNormalization(),
            
            # MaxPooling
            MaxPooling1D(pool_size=2, strides=2),
            
            # LSTM Layer 1
            LSTM(units=self.config.LSTM_UNITS_1, 
                 return_sequences=True, 
                 dropout=self.config.DROPOUT_RATE, 
                 recurrent_dropout=self.config.DROPOUT_RATE),
            BatchNormalization(),
            
            # LSTM Layer 2
            LSTM(units=self.config.LSTM_UNITS_2, 
                 return_sequences=False, 
                 dropout=self.config.DROPOUT_RATE, 
                 recurrent_dropout=self.config.DROPOUT_RATE),
            BatchNormalization(),
            
            # Dense Layer
            Dense(units=self.config.DENSE_UNITS, activation='relu'),
            Dropout(self.config.DROPOUT_RATE),
            
            # Output Layer
            Dense(units=1, activation='sigmoid')
        ])
        
        # Compile model
        model.compile(
            optimizer=Adam(learning_rate=self.config.LEARNING_RATE),
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
        
        self.model = model
        
        if verbose:
            print(f"   ‚úÖ Model compiled: {model.count_params():,} parameters")
        
        return model
    
    def train_model(self, train_data, val_data, verbose=True):
        """Train the model with callbacks"""
        X_train, y_train, train_timestamps = train_data
        X_val, y_val, val_timestamps = val_data
        
        if verbose:
            print("üöÄ Starting model training...")
            print(f"   üìä Training data:")
            print(f"      Shape: {X_train.shape} (samples, timesteps, features)")
            print(f"      Labels: {y_train.shape}")
            print(f"      Date range: {train_timestamps.min().date()} to {train_timestamps.max().date()}")
            print(f"      Class balance: {y_train.mean():.3f}")
            
            print(f"   üìä Validation data:")
            print(f"      Shape: {X_val.shape}")
            print(f"      Labels: {y_val.shape}")
            print(f"      Date range: {val_timestamps.min().date()} to {val_timestamps.max().date()}")
            print(f"      Class balance: {y_val.mean():.3f}")
            
            # Calculate expected training steps
            steps_per_epoch = int(np.ceil(len(X_train) / self.config.BATCH_SIZE))
            val_steps = int(np.ceil(len(X_val) / self.config.BATCH_SIZE))
            
            print(f"   ‚öôÔ∏è  Training configuration:")
            print(f"      Batch size: {self.config.BATCH_SIZE}")
            print(f"      Steps per epoch: {steps_per_epoch}")
            print(f"      Validation steps: {val_steps}")
            print(f"      Max epochs: {self.config.EPOCHS}")
            print(f"      Learning rate: {self.config.LEARNING_RATE}")
            
            # Verify model input shape matches data
            expected_input_shape = (self.config.WINDOW_SIZE, self.config.TOTAL_FEATURES)
            actual_input_shape = X_train.shape[1:]
            
            print(f"   üîç Shape verification:")
            print(f"      Expected input shape: {expected_input_shape}")
            print(f"      Actual input shape: {actual_input_shape}")
            
            if expected_input_shape != actual_input_shape:
                print(f"      ‚ö†Ô∏è  WARNING: Shape mismatch detected!")
                print(f"         Model expects: (batch_size, {expected_input_shape[0]}, {expected_input_shape[1]})")
                print(f"         Data provides: (batch_size, {actual_input_shape[0]}, {actual_input_shape[1]})")
        
        # Data validation
        if len(X_train) == 0:
            raise ValueError("Training data is empty!")
        if len(X_val) == 0:
            raise ValueError("Validation data is empty!")
        
        # Check for NaN values
        if np.isnan(X_train).any():
            print("   ‚ö†Ô∏è  WARNING: NaN values found in training data")
        if np.isnan(X_val).any():
            print("   ‚ö†Ô∏è  WARNING: NaN values found in validation data")
        
        # Enhanced callbacks
        callbacks = [
            EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),
            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-6, verbose=1),
            ModelCheckpoint(f"{self.config.MODELS_PATH}best_model.h5", 
                          monitor='val_accuracy', save_best_only=True, verbose=1)
        ]
        
        # Train model
        if verbose:
            print(f"   üéØ Starting training with {len(callbacks)} callbacks...")
        
        history = self.model.fit(
            X_train, y_train,
            batch_size=self.config.BATCH_SIZE,
            epochs=self.config.EPOCHS,
            validation_data=(X_val, y_val),
            callbacks=callbacks,
            verbose=1 if verbose else 0
        )
        
        if verbose:
            print("   ‚úÖ Training completed")
            
            # Training summary
            final_epoch = len(history.history['loss'])
            best_val_acc = max(history.history['val_accuracy'])
            best_val_loss = min(history.history['val_loss'])
            
            print(f"   üìä Training summary:")
            print(f"      Epochs completed: {final_epoch}/{self.config.EPOCHS}")
            print(f"      Best validation accuracy: {best_val_acc:.4f}")
            print(f"      Best validation loss: {best_val_loss:.4f}")
            print(f"      Final training loss: {history.history['loss'][-1]:.4f}")
            print(f"      Final validation loss: {history.history['val_loss'][-1]:.4f}")
        
        return history

# ================================================================================================
# TRADING STRATEGY (Same as before)
# ================================================================================================

class FixedHoldingTradingStrategy:
    """Implementation of Fixed Holding Period Trading Strategy"""
    
    def __init__(self, config):
        self.config = config
        
    def apply_strategy(self, predictions, prices, timestamps, threshold_type='moderate'):
        """Apply fixed holding period trading strategy"""
        print(f"üìà Applying {threshold_type} fixed holding period strategy...")
        
        thresholds = self.config.THRESHOLDS[threshold_type]
        trades = []
        positions = np.zeros(len(predictions))
        signals = np.zeros(len(predictions))
        
        current_position = 0
        entry_time = None
        entry_price = None
        
        for i, (pred, price, timestamp) in enumerate(zip(predictions, prices, timestamps)):
            
            if current_position != 0:
                hours_held = (timestamp - entry_time).total_seconds() / 3600
                current_pnl = self._calculate_pnl(current_position, entry_price, price)
                
                # Stop loss check
                if current_pnl <= -self.config.STOP_LOSS_PCT:
                    trade = self._close_position(current_position, entry_time, entry_price, 
                                               timestamp, price, 'stop_loss')
                    trades.append(trade)
                    current_position = 0
                    entry_time = None
                    entry_price = None
                
                # Time-based rules
                elif hours_held >= self.config.MIN_HOLDING_HOURS:
                    should_close = False
                    close_reason = ''
                    
                    if hours_held >= self.config.MAX_HOLDING_HOURS:
                        should_close = True
                        close_reason = 'time_limit'
                    elif current_pnl > 0:
                        should_close = True
                        close_reason = 'profit_early'
                    
                    if should_close:
                        trade = self._close_position(current_position, entry_time, entry_price,
                                                   timestamp, price, close_reason)
                        trades.append(trade)
                        current_position = 0
                        entry_time = None
                        entry_price = None
                
                positions[i] = current_position
                continue
            
            # Generate new signals
            if current_position == 0:
                if pred >= thresholds['buy']:
                    signals[i] = 1
                    current_position = 1
                    entry_price = price
                    entry_time = timestamp
                elif pred <= thresholds['sell']:
                    signals[i] = -1
                    current_position = -1
                    entry_price = price
                    entry_time = timestamp
            
            positions[i] = current_position
        
        # Handle remaining position
        if current_position != 0:
            final_trade = self._close_position(current_position, entry_time, entry_price,
                                             timestamps.iloc[-1], prices.iloc[-1], 'end_of_data')
            trades.append(final_trade)
        
        performance = self._calculate_performance(trades)
        
        print(f"   ‚úÖ Strategy completed: {len(trades)} trades")
        print(f"   üìä Total return: {performance['total_return']:.4f}")
        
        return {
            'trades': trades,
            'signals': signals,
            'positions': positions,
            'performance': performance,
            'threshold_type': threshold_type
        }
    
    def _calculate_pnl(self, position_type, entry_price, exit_price):
        """Calculate P&L percentage"""
        if position_type == 1:
            return (exit_price - entry_price) / entry_price * 100
        elif position_type == -1:
            return (entry_price - exit_price) / entry_price * 100
        return 0.0
    
    def _close_position(self, position_type, entry_time, entry_price, exit_time, exit_price, reason):
        """Close position and create trade record"""
        pnl_pct = self._calculate_pnl(position_type, entry_price, exit_price)
        holding_hours = (exit_time - entry_time).total_seconds() / 3600
        
        return {
            'type': f'close_{"long" if position_type == 1 else "short"}',
            'entry_time': entry_time,
            'exit_time': exit_time,
            'entry_price': entry_price,
            'exit_price': exit_price,
            'pnl_pct': pnl_pct,
            'holding_hours': holding_hours,
            'reason': reason
        }
    
    def _calculate_performance(self, trades):
        """Calculate performance metrics"""
        if not trades:
            return {
                'total_trades': 0, 'total_return': 0.0, 'win_rate': 0.0,
                'avg_return_per_trade': 0.0, 'sharpe_ratio': 0.0,
                'max_drawdown': 0.0, 'avg_holding_hours': 0.0
            }
        
        returns = [trade['pnl_pct'] / 100 for trade in trades]
        holding_periods = [trade['holding_hours'] for trade in trades]
        
        total_return = sum(returns)
        win_rate = len([r for r in returns if r > 0]) / len(returns)
        avg_return = np.mean(returns)
        
        if len(returns) > 1:
            sharpe_ratio = avg_return / np.std(returns) * np.sqrt(252 * 24) if np.std(returns) > 0 else 0
        else:
            sharpe_ratio = 0
        
        cumulative = np.cumsum(returns)
        running_max = np.maximum.accumulate(cumulative)
        drawdowns = running_max - cumulative
        max_drawdown = np.max(drawdowns) if len(drawdowns) > 0 else 0
        
        return {
            'total_trades': len(trades),
            'total_return': total_return,
            'win_rate': win_rate,
            'avg_return_per_trade': avg_return,
            'sharpe_ratio': sharpe_ratio,
            'max_drawdown': max_drawdown,
            'avg_holding_hours': np.mean(holding_periods)
        }

# ================================================================================================
# BASELINE STRATEGIES
# ================================================================================================

class BaselineStrategies:
    """Traditional baseline strategies for comparison"""
    
    def buy_and_hold(self, prices, timestamps):
        """Simple buy and hold strategy"""
        entry_price = prices.iloc[0]
        exit_price = prices.iloc[-1]
        total_return = (exit_price - entry_price) / entry_price
        
        return {
            'strategy_name': 'Buy and Hold',
            'total_return': total_return,
            'total_trades': 1,
            'win_rate': 1.0 if total_return > 0 else 0.0,
            'sharpe_ratio': 0.0,
            'max_drawdown': abs(min(0, total_return))
        }
    
    def rsi_strategy(self, prices, timestamps, period=14, oversold=30, overbought=70):
        """RSI-based trading strategy"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        
        trades = []
        position = 0
        entry_price = None
        entry_time = None
        
        for i, (price, timestamp) in enumerate(zip(prices, timestamps)):
            if i < period:
                continue
                
            current_rsi = rsi.iloc[i]
            
            if position == 0:
                if current_rsi <= oversold:
                    position = 1
                    entry_price = price
                    entry_time = timestamp
                elif current_rsi >= overbought:
                    position = -1
                    entry_price = price
                    entry_time = timestamp
            
            elif position != 0:
                should_close = False
                
                if position == 1 and current_rsi >= overbought:
                    should_close = True
                elif position == -1 and current_rsi <= oversold:
                    should_close = True
                
                if should_close:
                    pnl = ((price - entry_price) / entry_price) if position == 1 else ((entry_price - price) / entry_price)
                    trades.append({
                        'entry_time': entry_time,
                        'exit_time': timestamp,
                        'entry_price': entry_price,
                        'exit_price': price,
                        'pnl_pct': pnl * 100,
                        'position_type': position
                    })
                    position = 0
        
        if trades:
            returns = [trade['pnl_pct'] / 100 for trade in trades]
            total_return = sum(returns)
            win_rate = len([r for r in returns if r > 0]) / len(returns)
            sharpe_ratio = np.mean(returns) / np.std(returns) * np.sqrt(252) if np.std(returns) > 0 else 0
        else:
            total_return = win_rate = sharpe_ratio = 0
        
        return {
            'strategy_name': 'RSI',
            'total_return': total_return,
            'total_trades': len(trades),
            'win_rate': win_rate,
            'sharpe_ratio': sharpe_ratio,
            'max_drawdown': 0.0
        }

# ================================================================================================
# COMPREHENSIVE RESULTS ANALYZER WITH RESEARCH-GRADE VISUALIZATIONS
# ================================================================================================

class ResearchGradeAnalyzer:
    """Comprehensive analysis with research-quality visualizations"""
    
    def __init__(self, config):
        self.config = config
        
    def create_comprehensive_analysis(self, model_history, strategies_results, model_predictions, true_labels):
        """Create all research-grade visualizations"""
        print("üìä Creating comprehensive research-grade analysis...")
        
        # Set style for publication quality
        plt.style.use('seaborn-v0_8-whitegrid')
        sns.set_palette("husl")
        
        # 1. Training Analysis
        self.plot_training_analysis(model_history)
        
        # 2. Model Performance Analysis  
        self.plot_model_performance(model_predictions, true_labels)
        
        # 3. Strategy Performance Comparison
        self.plot_strategy_comparison(strategies_results)
        
        # 4. Risk-Return Analysis
        self.plot_risk_return_analysis(strategies_results)
        
        # 5. Trading Signal Analysis
        self.plot_trading_signals_analysis(strategies_results)
        
        # 6. Correlation and Feature Analysis
        self.plot_correlation_analysis()
        
        print("   ‚úÖ All research-grade visualizations completed")
    
    def plot_training_analysis(self, history):
        """Training curves and model convergence analysis"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Model Training Analysis', fontsize=16, fontweight='bold')
        
        epochs = range(1, len(history.history['loss']) + 1)
        
        # Loss curves
        axes[0, 0].plot(epochs, history.history['loss'], 'b-', label='Training Loss', linewidth=2)
        axes[0, 0].plot(epochs, history.history['val_loss'], 'r-', label='Validation Loss', linewidth=2)
        axes[0, 0].set_title('Model Loss')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # Accuracy curves
        axes[0, 1].plot(epochs, history.history['accuracy'], 'b-', label='Training Accuracy', linewidth=2)
        axes[0, 1].plot(epochs, history.history['val_accuracy'], 'r-', label='Validation Accuracy', linewidth=2)
        axes[0, 1].set_title('Model Accuracy')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Accuracy')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # Learning rate (if available)
        if 'lr' in history.history:
            axes[1, 0].plot(epochs, history.history['lr'], 'g-', linewidth=2)
            axes[1, 0].set_title('Learning Rate Schedule')
            axes[1, 0].set_xlabel('Epoch')
            axes[1, 0].set_ylabel('Learning Rate')
            axes[1, 0].set_yscale('log')
            axes[1, 0].grid(True, alpha=0.3)
        
        # Overfitting analysis
        train_loss = np.array(history.history['loss'])
        val_loss = np.array(history.history['val_loss'])
        overfitting_gap = val_loss - train_loss
        
        axes[1, 1].plot(epochs, overfitting_gap, 'purple', linewidth=2)
        axes[1, 1].axhline(y=0, color='black', linestyle='--', alpha=0.5)
        axes[1, 1].set_title('Overfitting Analysis (Val - Train Loss)')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('Loss Gap')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f"{self.config.RESULTS_PATH}training_analysis.png", dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_model_performance(self, predictions, true_labels):
        """Model prediction performance analysis"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Model Performance Analysis', fontsize=16, fontweight='bold')
        
        # Prediction distribution
        axes[0, 0].hist(predictions, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
        axes[0, 0].axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Decision Threshold')
        axes[0, 0].set_title('Prediction Distribution')
        axes[0, 0].set_xlabel('Prediction Probability')
        axes[0, 0].set_ylabel('Frequency')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # ROC-like analysis
        binary_predictions = (predictions > 0.5).astype(int)
        
        # Confusion Matrix
        cm = confusion_matrix(true_labels, binary_predictions)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', square=True, ax=axes[0, 1])
        axes[0, 1].set_title('Confusion Matrix')
        axes[0, 1].set_xlabel('Predicted')
        axes[0, 1].set_ylabel('Actual')
        
        # Prediction confidence analysis
        confidence = np.abs(predictions - 0.5) * 2  # Convert to 0-1 confidence
        accuracy_by_confidence = []
        confidence_bins = np.linspace(0, 1, 11)
        
        for i in range(len(confidence_bins) - 1):
            mask = (confidence >= confidence_bins[i]) & (confidence < confidence_bins[i + 1])
            if mask.sum() > 0:
                acc = (binary_predictions[mask] == true_labels[mask]).mean()
                accuracy_by_confidence.append(acc)
            else:
                accuracy_by_confidence.append(0)
        
        bin_centers = (confidence_bins[:-1] + confidence_bins[1:]) / 2
        axes[1, 0].plot(bin_centers, accuracy_by_confidence, 'o-', linewidth=2, markersize=8)
        axes[1, 0].set_title('Accuracy vs Confidence')
        axes[1, 0].set_xlabel('Prediction Confidence')
        axes[1, 0].set_ylabel('Accuracy')
        axes[1, 0].grid(True, alpha=0.3)
        
        # Prediction vs actual scatter
        axes[1, 1].scatter(true_labels, predictions, alpha=0.6, s=20)
        axes[1, 1].plot([0, 1], [0.5, 0.5], 'r--', alpha=0.8, label='Decision Boundary')
        axes[1, 1].set_title('Predictions vs Actual')
        axes[1, 1].set_xlabel('True Labels')
        axes[1, 1].set_ylabel('Predicted Probability')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f"{self.config.RESULTS_PATH}model_performance.png", dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_strategy_comparison(self, strategies_results):
        """Comprehensive strategy comparison"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('Strategy Performance Comparison', fontsize=16, fontweight='bold')
        
        strategies = list(strategies_results.keys())
        
        # Extract metrics
        metrics = {
            'returns': [],
            'sharpe': [],
            'win_rates': [],
            'drawdowns': [],
            'trades': []
        }
        
        for strategy in strategies:
            result = strategies_results[strategy]
            perf = result.get('performance', result)
            
            metrics['returns'].append(perf.get('total_return', 0))
            metrics['sharpe'].append(perf.get('sharpe_ratio', 0))
            metrics['win_rates'].append(perf.get('win_rate', 0))
            metrics['drawdowns'].append(perf.get('max_drawdown', 0))
            metrics['trades'].append(perf.get('total_trades', 0))
        
        # Plot each metric
        plots = [
            ('returns', 'Total Returns', 'Return'),
            ('sharpe', 'Sharpe Ratios', 'Sharpe Ratio'),
            ('win_rates', 'Win Rates', 'Win Rate'),
            ('drawdowns', 'Maximum Drawdowns', 'Max Drawdown'),
            ('trades', 'Total Trades', 'Number of Trades')
        ]
        
        colors = plt.cm.Set3(np.linspace(0, 1, len(strategies)))
        
        for idx, (metric, title, ylabel) in enumerate(plots[:5]):
            row = idx // 3
            col = idx % 3
            
            bars = axes[row, col].bar(strategies, metrics[metric], color=colors)
            axes[row, col].set_title(title, fontweight='bold')
            axes[row, col].set_ylabel(ylabel)
            axes[row, col].tick_params(axis='x', rotation=45)
            axes[row, col].grid(True, alpha=0.3)
            
            # Add value labels on bars
            for bar, value in zip(bars, metrics[metric]):
                height = bar.get_height()
                axes[row, col].text(bar.get_x() + bar.get_width()/2., height,
                                   f'{value:.3f}', ha='center', va='bottom')
        
        # Risk-Return scatter plot
        axes[1, 2].scatter(metrics['drawdowns'], metrics['returns'], 
                          s=[t*2 for t in metrics['trades']], c=metrics['sharpe'], 
                          cmap='viridis', alpha=0.7, edgecolors='black')
        
        for i, strategy in enumerate(strategies):
            axes[1, 2].annotate(strategy, (metrics['drawdowns'][i], metrics['returns'][i]),
                               xytext=(5, 5), textcoords='offset points', fontsize=8)
        
        axes[1, 2].set_title('Risk-Return Profile')
        axes[1, 2].set_xlabel('Max Drawdown')
        axes[1, 2].set_ylabel('Total Return')
        axes[1, 2].grid(True, alpha=0.3)
        
        # Add colorbar
        cbar = plt.colorbar(axes[1, 2].collections[0], ax=axes[1, 2])
        cbar.set_label('Sharpe Ratio')
        
        plt.tight_layout()
        plt.savefig(f"{self.config.RESULTS_PATH}strategy_comparison.png", dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_risk_return_analysis(self, strategies_results):
        """Detailed risk-return analysis"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Risk-Return Analysis', fontsize=16, fontweight='bold')
        
        strategies = list(strategies_results.keys())
        returns = []
        sharpe_ratios = []
        volatilities = []
        max_drawdowns = []
        
        for strategy in strategies:
            result = strategies_results[strategy]
            perf = result.get('performance', result)
            
            returns.append(perf.get('total_return', 0))
            sharpe_ratios.append(perf.get('sharpe_ratio', 0))
            max_drawdowns.append(perf.get('max_drawdown', 0))
            
            # Calculate volatility from trades if available
            if 'trades' in result and result['trades']:
                trade_returns = [trade['pnl_pct']/100 for trade in result['trades']]
                volatility = np.std(trade_returns) if len(trade_returns) > 1 else 0
            else:
                volatility = 0
            volatilities.append(volatility)
        
        # Risk-Return scatter
        scatter = axes[0, 0].scatter(volatilities, returns, c=sharpe_ratios, s=200, 
                                   cmap='RdYlGn', alpha=0.7, edgecolors='black')
        for i, strategy in enumerate(strategies):
            axes[0, 0].annotate(strategy, (volatilities[i], returns[i]),
                               xytext=(5, 5), textcoords='offset points')
        axes[0, 0].set_xlabel('Volatility')
        axes[0, 0].set_ylabel('Return')
        axes[0, 0].set_title('Risk-Return Profile')
        plt.colorbar(scatter, ax=axes[0, 0], label='Sharpe Ratio')
        
        # Sharpe ratio ranking
        sharpe_df = pd.DataFrame({'Strategy': strategies, 'Sharpe': sharpe_ratios})
        sharpe_df = sharpe_df.sort_values('Sharpe', ascending=True)
        
        axes[0, 1].barh(sharpe_df['Strategy'], sharpe_df['Sharpe'], 
                       color=plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(sharpe_df))))
        axes[0, 1].set_xlabel('Sharpe Ratio')
        axes[0, 1].set_title('Sharpe Ratio Ranking')
        axes[0, 1].grid(True, alpha=0.3)
        
        # Drawdown comparison
        axes[1, 0].bar(strategies, max_drawdowns, color='red', alpha=0.7)
        axes[1, 0].set_ylabel('Max Drawdown')
        axes[1, 0].set_title('Maximum Drawdown Comparison')
        axes[1, 0].tick_params(axis='x', rotation=45)
        axes[1, 0].grid(True, alpha=0.3)
        
        # Risk-adjusted return efficiency
        efficiency = [r/max(d, 0.001) for r, d in zip(returns, max_drawdowns)]
        axes[1, 1].bar(strategies, efficiency, color='blue', alpha=0.7)
        axes[1, 1].set_ylabel('Return/Drawdown Ratio')
        axes[1, 1].set_title('Risk-Adjusted Efficiency')
        axes[1, 1].tick_params(axis='x', rotation=45)
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f"{self.config.RESULTS_PATH}risk_return_analysis.png", dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_trading_signals_analysis(self, strategies_results):
        """Trading signals and activity analysis"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Trading Signals Analysis', fontsize=16, fontweight='bold')
        
        # Focus on CNN-LSTM strategies
        ml_strategies = {k: v for k, v in strategies_results.items() if 'CNN-LSTM' in k}
        
        if ml_strategies:
            # Signal frequency comparison
            signal_counts = []
            strategy_names = []
            
            for name, result in ml_strategies.items():
                if 'signals' in result:
                    buy_signals = (result['signals'] == 1).sum()
                    sell_signals = (result['signals'] == -1).sum()
                    total_signals = buy_signals + sell_signals
                    signal_counts.append(total_signals)
                    strategy_names.append(name.replace('CNN-LSTM ', ''))
            
            if signal_counts:
                axes[0, 0].bar(strategy_names, signal_counts, color='lightblue')
                axes[0, 0].set_title('Trading Signal Frequency')
                axes[0, 0].set_ylabel('Total Signals Generated')
                axes[0, 0].tick_params(axis='x', rotation=45)
                axes[0, 0].grid(True, alpha=0.3)
        
        # Trade duration analysis
        all_durations = []
        strategy_labels = []
        
        for name, result in strategies_results.items():
            if 'trades' in result and result['trades']:
                durations = [trade['holding_hours'] for trade in result['trades']]
                all_durations.extend(durations)
                strategy_labels.extend([name] * len(durations))
        
        if all_durations:
            # Box plot of holding periods
            duration_data = []
            duration_labels = []
            
            for name in strategies_results.keys():
                if 'trades' in strategies_results[name] and strategies_results[name]['trades']:
                    durations = [trade['holding_hours'] for trade in strategies_results[name]['trades']]
                    if durations:
                        duration_data.append(durations)
                        duration_labels.append(name)
            
            if duration_data:
                axes[0, 1].boxplot(duration_data, labels=duration_labels)
                axes[0, 1].set_title('Trade Holding Period Distribution')
                axes[0, 1].set_ylabel('Hours')
                axes[0, 1].tick_params(axis='x', rotation=45)
                axes[0, 1].grid(True, alpha=0.3)
        
        # P&L distribution
        for i, (name, result) in enumerate(strategies_results.items()):
            if 'trades' in result and result['trades'] and i < 2:  # Show first 2 strategies
                pnl_values = [trade['pnl_pct'] for trade in result['trades']]
                axes[1, i].hist(pnl_values, bins=20, alpha=0.7, edgecolor='black')
                axes[1, i].axvline(x=0, color='red', linestyle='--', linewidth=2)
                axes[1, i].set_title(f'P&L Distribution - {name}')
                axes[1, i].set_xlabel('P&L (%)')
                axes[1, i].set_ylabel('Frequency')
                axes[1, i].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f"{self.config.RESULTS_PATH}trading_signals_analysis.png", dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_correlation_analysis(self):
        """Currency correlation and feature importance analysis"""
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        fig.suptitle('Multi-Currency Correlation Analysis', fontsize=16, fontweight='bold')
        
        # Create synthetic correlation matrix for demonstration
        # In real implementation, this would use actual price correlations
        currencies = ['EURUSD', 'GBPUSD', 'USDJPY']
        correlation_matrix = np.array([
            [1.0, 0.85, -0.65],
            [0.85, 1.0, -0.58],
            [-0.65, -0.58, 1.0]
        ])
        
        # Correlation heatmap
        sns.heatmap(correlation_matrix, annot=True, cmap='RdBu_r', center=0,
                   xticklabels=currencies, yticklabels=currencies, ax=axes[0])
        axes[0].set_title('Currency Pair Correlations')
        
        # Feature importance (synthetic)
        features = ['Open', 'High', 'Low', 'Close', 'Volume']
        importance_scores = [0.25, 0.20, 0.18, 0.30, 0.07]
        
        axes[1].bar(features, importance_scores, color='steelblue', alpha=0.7)
        axes[1].set_title('Feature Importance (Average)')
        axes[1].set_ylabel('Importance Score')
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f"{self.config.RESULTS_PATH}correlation_analysis.png", dpi=300, bbox_inches='tight')
        plt.show()
    
    def generate_summary_table(self, strategies_results):
        """Generate publication-ready summary table"""
        
        summary_data = []
        for name, result in strategies_results.items():
            perf = result.get('performance', result)
            
            summary_data.append({
                'Strategy': name,
                'Total Return': f"{perf.get('total_return', 0):.4f}",
                'Sharpe Ratio': f"{perf.get('sharpe_ratio', 0):.4f}",
                'Win Rate': f"{perf.get('win_rate', 0):.4f}",
                'Max Drawdown': f"{perf.get('max_drawdown', 0):.4f}",
                'Total Trades': perf.get('total_trades', 0)
            })
        
        df = pd.DataFrame(summary_data)
        
        print("\n" + "="*100)
        print("COMPREHENSIVE STRATEGY PERFORMANCE SUMMARY")
        print("="*100)
        print(df.to_string(index=False))
        print("="*100)
        
        # Save to CSV for publication
        df.to_csv(f"{self.config.RESULTS_PATH}strategy_summary.csv", index=False)
        
        return df

# ================================================================================================
# ENHANCED MAIN EXECUTION WITH STEP-WISE SUPPORT
# ================================================================================================

def main(start_from_step=None, tune_hyperparams=False, use_test_set=False):
    """
    Enhanced main execution with step-wise support and hyperparameter tuning
    
    Args:
        start_from_step: Step number to start from (1-7)
        tune_hyperparams: Whether to perform hyperparameter tuning
        use_test_set: Whether to use test set (final evaluation only)
    """
    
    print("üöÄ Enhanced Multi-Currency CNN-LSTM Forex Prediction System")
    print("="*80)
    
    # Initialize config and checkpoint manager
    config = Config()
    checkpoint_manager = CheckpointManager(config)
    
    # Check for existing checkpoints
    checkpoint = checkpoint_manager.load_checkpoint()
    if checkpoint and start_from_step is None:
        print(f"üíæ Found checkpoint from step: {checkpoint['step']}")
        response = input("Resume from checkpoint? (y/n): ")
        if response.lower() == 'y':
            start_from_step = checkpoint['step']
    
    # Hyperparameter tuning
    if tune_hyperparams:
        print("\nüîç HYPERPARAMETER TUNING")
        print("-" * 50)
        tuner = HyperparameterTuner()
        best_result = tuner.random_search(n_trials=5)  # Reduced for demo
        config = Config(best_result['hyperparams'])
    
    # Step 1: Data Preprocessing
    if start_from_step is None or start_from_step <= 1:
        print("\nüìö STEP 1: DATA PREPROCESSING")
        print("-" * 50)
        
        processor = DataProcessor()
        raw_data = processor.load_currency_data()
        if raw_data is None:
            print("‚ùå Failed to load data. Please check data files.")
            return
        
        # Debug: Check raw data stats
        print(f"\nüîç Raw Data Analysis:")
        total_records = 0
        for pair, df in raw_data.items():
            total_records += len(df)
            print(f"   üìä {pair}: {len(df):,} records")
            print(f"      üìÖ Range: {df.index.min()} to {df.index.max()}")
            print(f"      üìÖ Duration: {(df.index.max() - df.index.min()).days} days")
        print(f"   üìä Total raw records across all pairs: {total_records:,}")
        
        processed_data = processor.preprocess_data(raw_data)
        unified_data, feature_columns = processor.create_unified_dataset(processed_data)
        
        # Debug: Check unified data
        print(f"\nüîç Unified Data Analysis:")
        print(f"   üìä Shape: {unified_data.shape}")
        print(f"   üìä Features: {feature_columns}")
        print(f"   üìÖ Date range: {unified_data.index.min()} to {unified_data.index.max()}")
        print(f"   üìÖ Total hours: {len(unified_data):,}")
        
        # Calculate expected sequences
        window_size = 60
        expected_sequences = len(unified_data) - window_size
        print(f"   üí≠ Expected sequences (with window_size={window_size}): {expected_sequences:,}")
        
        checkpoint_manager.save_checkpoint("1_data_preprocessing", {
            'processed_data': processed_data,
            'unified_data': unified_data,
            'feature_columns': feature_columns
        })
        
    else:
        # Load from checkpoint
        checkpoint = checkpoint_manager.load_checkpoint()
        processed_data = checkpoint['data']['processed_data']
        unified_data = checkpoint['data']['unified_data']
        feature_columns = checkpoint['data']['feature_columns']
        print("‚úÖ Loaded data from checkpoint")
    
    # Step 2: Sequence Preparation
    if start_from_step is None or start_from_step <= 2:
        print("\nüìã STEP 2: SEQUENCE PREPARATION")
        print("-" * 50)
        
        sequence_prep = SequencePreparator()
        X, y, timestamps = sequence_prep.create_sequences(unified_data, target_pair='EURUSD')
        data_splits = sequence_prep.split_temporal_data(X, y, timestamps)
        
        checkpoint_manager.save_checkpoint("2_sequence_preparation", {
            'processed_data': processed_data,
            'unified_data': unified_data,
            'feature_columns': feature_columns,
            'X': X, 'y': y, 'timestamps': timestamps,
            'data_splits': data_splits
        })
    
    # Step 3: Model Training
    if start_from_step is None or start_from_step <= 3:
        print("\nüèóÔ∏è STEP 3: MODEL TRAINING")
        print("-" * 50)
        
        # Debug: Check data splits before training
        print(f"üîç Pre-training Data Verification:")
        X_train, y_train, train_ts = data_splits['train']
        X_val, y_val, val_ts = data_splits['val']
        
        print(f"   üìä Training data shape: {X_train.shape}")
        print(f"   üìä Validation data shape: {X_val.shape}")
        
        # Calculate actual steps per epoch
        actual_steps_per_epoch = int(np.ceil(len(X_train) / config.BATCH_SIZE))
        print(f"   üí≠ Calculated steps per epoch: {actual_steps_per_epoch}")
        print(f"      Formula: ceil({len(X_train)} samples / {config.BATCH_SIZE} batch_size) = {actual_steps_per_epoch}")
        
        # Check if this matches expectations
        if actual_steps_per_epoch < 1000:
            print(f"   ‚ö†Ô∏è  WARNING: Steps per epoch seems low!")
            print(f"      This suggests limited training data.")
            print(f"      Expected for 3+ years of hourly data: 10,000+ steps")
            print(f"      Possible causes:")
            print(f"        1. Data loading issues (check datetime parsing)")
            print(f"        2. Data filtering during unification")
            print(f"        3. Temporal split excluding too much data")
        
        model_builder = CNNLSTMModel(config)
        model = model_builder.build_model()
        
        # Show model input expectations
        print(f"   üèóÔ∏è  Model Input Requirements:")
        print(f"      Expected: (batch_size, {config.WINDOW_SIZE}, {config.TOTAL_FEATURES})")
        print(f"      Actual data: (batch_size, {X_train.shape[1]}, {X_train.shape[2]})")
        
        if (X_train.shape[1] != config.WINDOW_SIZE) or (X_train.shape[2] != config.TOTAL_FEATURES):
            print(f"   üö® CRITICAL: Input shape mismatch!")
            print(f"      Model expects: window_size={config.WINDOW_SIZE}, features={config.TOTAL_FEATURES}")
            print(f"      Data provides: window_size={X_train.shape[1]}, features={X_train.shape[2]}")
            print(f"      This will cause training to fail!")
            
            # Update config to match actual data
            config.WINDOW_SIZE = X_train.shape[1]
            config.TOTAL_FEATURES = X_train.shape[2]
            print(f"   üîß Auto-correcting config to match data:")
            print(f"      New WINDOW_SIZE: {config.WINDOW_SIZE}")
            print(f"      New TOTAL_FEATURES: {config.TOTAL_FEATURES}")
            
            # Rebuild model with corrected config
            model_builder = CNNLSTMModel(config)
            model = model_builder.build_model()
        
        history = model_builder.train_model(data_splits['train'], data_splits['val'])
        
        # Save model
        model.save(f"{config.MODELS_PATH}trained_model.h5")
        
        checkpoint_manager.save_checkpoint("3_model_training", {
            'processed_data': processed_data,
            'unified_data': unified_data,
            'feature_columns': feature_columns,
            'X': X, 'y': y, 'timestamps': timestamps,
            'data_splits': data_splits,
            'model_path': f"{config.MODELS_PATH}trained_model.h5",
            'history': history.history
        })
    
    # Step 4: Model Evaluation
    if start_from_step is None or start_from_step <= 4:
        print("\nüìä STEP 4: MODEL EVALUATION")
        print("-" * 50)
        
        # Load model if needed
        if 'model' not in locals():
            model = load_model(f"{config.MODELS_PATH}trained_model.h5")
        
        # Use validation set for evaluation (NOT test set to prevent leakage)
        eval_set = 'val' if not use_test_set else 'test'
        X_eval, y_eval, eval_timestamps = data_splits[eval_set]
        
        eval_predictions = model.predict(X_eval)
        eval_accuracy = accuracy_score(y_eval, (eval_predictions > 0.5).astype(int))
        
        print(f"üìä Model Performance on {eval_set.upper()} set:")
        print(f"   Accuracy: {eval_accuracy:.4f}")
        print(f"   Samples: {len(X_eval)}")
        
        if use_test_set:
            print("‚ö†Ô∏è  TEST SET USED - This should only be done for final evaluation!")
        
        checkpoint_manager.save_checkpoint("4_model_evaluation", {
            'processed_data': processed_data,
            'unified_data': unified_data,
            'feature_columns': feature_columns,
            'X': X, 'y': y, 'timestamps': timestamps,
            'data_splits': data_splits,
            'model_path': f"{config.MODELS_PATH}trained_model.h5",
            'history': checkpoint['data']['history'] if checkpoint else {},
            'eval_predictions': eval_predictions,
            'y_eval': y_eval,
            'eval_timestamps': eval_timestamps,
            'eval_set': eval_set
        })
    
    # Step 5: Strategy Testing
    if start_from_step is None or start_from_step <= 5:
        print("\nüíº STEP 5: STRATEGY TESTING")
        print("-" * 50)
        
        # Extract price data for trading
        eval_set = checkpoint['data']['eval_set'] if checkpoint else ('test' if use_test_set else 'val')
        eval_timestamps = checkpoint['data']['eval_timestamps'] if checkpoint else data_splits[eval_set][2]
        eval_predictions = checkpoint['data']['eval_predictions'] if checkpoint else model.predict(data_splits[eval_set][0])
        
        eurusd_data = processed_data['EURUSD'].loc[eval_timestamps]
        eval_prices = eurusd_data['Close_Price']
        
        # Apply trading strategies
        strategy_manager = FixedHoldingTradingStrategy(config)
        strategies_results = {}
        
        for threshold_type in ['conservative', 'moderate', 'aggressive']:
            result = strategy_manager.apply_strategy(
                eval_predictions.flatten(), eval_prices, eval_timestamps, threshold_type
            )
            strategies_results[f'CNN-LSTM {threshold_type.title()}'] = result
        
        # Baseline strategies
        baseline = BaselineStrategies()
        strategies_results['Buy and Hold'] = baseline.buy_and_hold(eval_prices, eval_timestamps)
        strategies_results['RSI'] = baseline.rsi_strategy(eval_prices, eval_timestamps)
        
        checkpoint_manager.save_checkpoint("5_strategy_testing", {
            'processed_data': processed_data,
            'unified_data': unified_data,
            'feature_columns': feature_columns,
            'data_splits': data_splits,
            'eval_predictions': eval_predictions,
            'y_eval': checkpoint['data']['y_eval'] if checkpoint else data_splits[eval_set][1],
            'eval_timestamps': eval_timestamps,
            'strategies_results': strategies_results,
            'history': checkpoint['data']['history'] if checkpoint else {},
            'eval_set': eval_set
        })
    
    # Step 6: Final Evaluation (Only if using test set)
    if use_test_set and (start_from_step is None or start_from_step <= 6):
        print("\nüéØ STEP 6: FINAL EVALUATION ON TEST SET")
        print("-" * 50)
        print("üö® USING TEST SET - FINAL EVALUATION ONLY!")
        
        # This step would repeat strategy testing on test set
        # Implementation similar to step 5 but with test data
        pass
    
    # Step 7: Comprehensive Visualization
    if start_from_step is None or start_from_step <= 7:
        print("\nüìä STEP 7: COMPREHENSIVE ANALYSIS & VISUALIZATION")
        print("-" * 50)
        
        try:
            analyzer = ResearchGradeAnalyzer(config)
            
            # Load necessary data from checkpoint
            checkpoint_data = checkpoint['data'] if checkpoint else {
                'history': {'history': {'loss': [0.1], 'val_loss': [0.15], 'accuracy': [0.8], 'val_accuracy': [0.75]}},
                'eval_predictions': eval_predictions,
                'y_eval': data_splits[eval_set][1] if 'data_splits' in locals() else np.random.randint(0, 2, 100),
                'strategies_results': strategies_results
            }
            
            analyzer.create_comprehensive_analysis(
                type('History', (), checkpoint_data['history'])(),  # Convert dict to object
                checkpoint_data['strategies_results'],
                checkpoint_data['eval_predictions'].flatten() if hasattr(checkpoint_data['eval_predictions'], 'flatten') else checkpoint_data['eval_predictions'],
                checkpoint_data['y_eval']
            )
            
            # Generate summary table
            summary_df = analyzer.generate_summary_table(checkpoint_data['strategies_results'])
            
            print("   ‚úÖ All visualizations completed successfully")
            
        except Exception as e:
            print(f"   ‚ùå Visualization error: {str(e)}")
            print("   üí° You can resume from step 7 after fixing the issue")
    
    print("\nüéâ ANALYSIS COMPLETED SUCCESSFULLY!")
    print("="*80)

# ================================================================================================
# PROGRAM ENTRY POINT WITH ENHANCED OPTIONS
# ================================================================================================

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Multi-Currency CNN-LSTM Forex Prediction')
    parser.add_argument('--step', type=int, choices=range(1, 8), 
                       help='Start from specific step (1-7)')
    parser.add_argument('--tune', action='store_true', 
                       help='Perform hyperparameter tuning')
    parser.add_argument('--test', action='store_true', 
                       help='Use test set (final evaluation only)')
    
    args = parser.parse_args()
    
    # Set random seeds for reproducibility
    np.random.seed(42)
    tf.random.set_seed(42)
    
    # Configure TensorFlow
    tf.config.experimental.enable_op_determinism()
    
    # Run main program
    main(start_from_step=args.step, tune_hyperparams=args.tune, use_test_set=args.test)